{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeWeightsConstraint:\n",
    "    def __init__(self, start_index, end_index):\n",
    "        self.start_index = start_index\n",
    "        self.end_index = end_index\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            weight = module.weight\n",
    "            weight_data = weight.data\n",
    "            weight_data[0][0][0][self.start_index:self.end_index] = torch.clamp(weight_data[0][0][0][self.start_index:self.end_index], max=0.0)\n",
    "\n",
    "    def backward_hook(self, grad):\n",
    "        grad[..., self.start_index:self.end_index] = torch.where(\n",
    "            grad[..., self.start_index:self.end_index] > 0,\n",
    "            grad[..., self.start_index:self.end_index],\n",
    "            torch.zeros_like(grad[..., self.start_index:self.end_index])            \n",
    "        )\n",
    "        return grad\n",
    "    \n",
    "class ZeroWeightsConstraint:\n",
    "    def __init__(self, indices=[]):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __call__(self, module):\n",
    "        if hasattr(module, 'weight'):\n",
    "            weight = module.weight\n",
    "            weight_data = weight.data\n",
    "            weight_data[0][0][0][self.indices] = 0.0\n",
    "    \n",
    "    def backward_hook(self, grad):\n",
    "        grad[..., self.indices] = 0.0\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([32, 2, 30])\n",
      "After reshape:  torch.Size([32, 1, 2, 30])\n",
      "After conv1:  torch.Size([32, 1, 2, 1])\n",
      "After flatten:  torch.Size([32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN1(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNN1, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=(1, self.num_features), padding=0, bias=False)\n",
    "        self.conv1.apply(NegativeWeightsConstraint(start_index=0, end_index=3))\n",
    "        self.conv1.apply(ZeroWeightsConstraint(indices=[3,5,10,14,16,19,23,26,28]))\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        batch_size, num_choices, num_features = x.size()\n",
    "        if debug:\n",
    "            print(\"x.shape: \", x.shape)\n",
    "        x = x.view(batch_size, 1, num_choices, num_features)\n",
    "        if debug:\n",
    "            print(\"After reshape: \", x.shape) \n",
    "        x = self.conv1(x)\n",
    "        if debug:\n",
    "            print(\"After conv1: \", x.shape) \n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        if debug:\n",
    "            print(\"After flatten: \", x.shape)\n",
    "        return x\n",
    "num_features = 30\n",
    "model = CNN1(num_features).to(device)\n",
    "X = torch.rand(32, 2, num_features).to(device)\n",
    "model(X, debug=True).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After reshape:  torch.Size([32, 1, 2, 495])\n",
      "After conv1:  torch.Size([32, 1, 2, 1])\n",
      "After flatten:  torch.Size([32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CNN2(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(CNN2, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=(1, self.num_features*(self.num_features+1)//2+self.num_features), padding=0, bias=False)\n",
    "        self.conv1.apply(NegativeWeightsConstraint(start_index=0, end_index=3))\n",
    "        self.conv1.apply(ZeroWeightsConstraint(indices=[3,5,10,14,16,19,23,26,28]))\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        batch_size, num_choices, num_features = x.size()\n",
    "        x = x.unsqueeze(-1)\n",
    "        y = torch.matmul(x,x.transpose(-1,-2))\n",
    "        upper_triangular_indices = torch.triu_indices(num_features, num_features, offset=0)\n",
    "        upper_triangle = y[..., upper_triangular_indices[0], upper_triangular_indices[1]]\n",
    "        upper_triangle = upper_triangle.unsqueeze(-1)\n",
    "        y = torch.cat((x, upper_triangle), dim=-2)\n",
    "\n",
    "        y = y.view(batch_size, 1, num_choices, num_features*(num_features+1)//2+num_features)\n",
    "        if debug:\n",
    "            print(\"After reshape: \", y.shape) \n",
    "        y = self.conv1(y)\n",
    "        if debug:\n",
    "            print(\"After conv1: \", y.shape) \n",
    "        y = torch.flatten(y, start_dim=1)\n",
    "        if debug:\n",
    "            print(\"After flatten: \", y.shape)\n",
    "        return y\n",
    "num_features = 30\n",
    "model = CNN2(num_features).to(device)\n",
    "X = torch.rand(32, 2, num_features).to(device)\n",
    "model(X, debug=True).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After upper_triangle:  torch.Size([32, 2, 465, 1])\n",
      "After concat:  torch.Size([32, 2, 495, 1])\n",
      "After reshape:  torch.Size([64, 495])\n",
      "x:  torch.Size([64, 3])\n",
      "After concat:  torch.Size([64, 495])\n",
      "After transformer_encoder:  torch.Size([64, 495])\n",
      "After fc:  torch.Size([64, 495])\n",
      "After Transformer_encoder:  torch.Size([64, 495])\n",
      "After conv1:  torch.Size([32, 1, 2, 1])\n",
      "After flatten:  torch.Size([32, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.transformer_encoder = nn.TransformerEncoder(nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout), num_encoder_layers)\n",
    "        self.fc = nn.Linear(d_model, dim_feedforward)\n",
    "    \n",
    "    def forward(self, y, debug=False):\n",
    "        y = self.transformer_encoder(y)\n",
    "        if debug:\n",
    "            print(\"After transformer_encoder: \", y.shape)\n",
    "        y = self.fc(y)\n",
    "        if debug:\n",
    "            print(\"After fc: \", y.shape)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_features, hidden_size, d_model, conv_kernel, nhead, num_encoder_layers, dim_feedforward, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.num_features = num_features\n",
    "        self.num_features2 = num_features*(num_features+1)//2+num_features\n",
    "        self.hidden_size = hidden_size\n",
    "        self.d_model = d_model\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.nhead = nhead\n",
    "        self.num_encoder_layers = num_encoder_layers\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "        self.Transformer_encoder = TransformerEncoder(num_features, d_model, nhead, num_encoder_layers, dim_feedforward, dropout)\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=(1, self.conv_kernel), padding=0, bias=False)\n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(self.hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv1.apply(NegativeWeightsConstraint(start_index=0, end_index=3))\n",
    "\n",
    "    def forward(self, x, debug=False):\n",
    "        batch_size, num_choices, num_features = x.size()\n",
    "        x = x.unsqueeze(-1)\n",
    "        y = torch.matmul(x,x.transpose(-1,-2))\n",
    "        upper_triangular_indices = torch.triu_indices(num_features, num_features, offset=0)\n",
    "        upper_triangle = y[..., upper_triangular_indices[0], upper_triangular_indices[1]]\n",
    "        upper_triangle = upper_triangle.unsqueeze(-1)\n",
    "        if debug:\n",
    "            print(\"After upper_triangle: \", upper_triangle.shape)\n",
    "        y = torch.cat((x, upper_triangle), dim=-2)\n",
    "        if debug:\n",
    "            print(\"After concat: \", y.shape)\n",
    "        y = self.dropout(y).squeeze(-1)\n",
    "        # y = self.avgpool(y)\n",
    "        # if debug:\n",
    "        #     print(\"After avgpool: \", y.shape)\n",
    "        y = y.view(batch_size * num_choices, self.num_features2)\n",
    "        if debug:\n",
    "            print(\"After reshape: \", y.shape)\n",
    "        y = F.relu(y, inplace=True)\n",
    "\n",
    "        x = x.view(batch_size * num_choices, self.num_features)[:,:3]\n",
    "        if debug:\n",
    "            print(\"x: \", x.shape)\n",
    "        # y = torch.cat((x, y), dim=-1)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"After concat: \", y.shape) \n",
    "        y = self.Transformer_encoder(y, debug=debug)\n",
    "        if debug:\n",
    "            print(\"After Transformer_encoder: \", y.shape)   \n",
    "        y = torch.cat((x, y), dim=-1)\n",
    "        y = y.view(batch_size, 1, num_choices, self.conv_kernel)\n",
    "        y = self.conv1(y)\n",
    "        if debug:\n",
    "            print(\"After conv1: \", y.shape) \n",
    "        y = torch.flatten(y, start_dim=1)\n",
    "        if debug:\n",
    "            print(\"After flatten: \", y.shape)\n",
    "        return y\n",
    "num_features = 30\n",
    "num_features2 = num_features*(num_features+1)//2+num_features\n",
    "hidden_size = num_features2\n",
    "d_model = num_features2\n",
    "conv_kernel = num_features2+3\n",
    "nhead = 5\n",
    "num_encoder_layers = 4\n",
    "dim_feedforward = num_features2\n",
    "dropout = 0.2\n",
    "model = Transformer(num_features, hidden_size, d_model, conv_kernel, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
    "X = torch.rand(32, 2, num_features).to(device)\n",
    "model(X, debug=True).shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import Subset, SequentialSampler\n",
    "from torch.utils.data import ConcatDataset\n",
    "import pickle\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def PrepareData(data_path, data_file, num_route): \n",
    "    data = pickle.load(open(data_path + \"/\" + data_file, \"rb\"))\n",
    "\n",
    "    sampler = []\n",
    "    ds = []\n",
    "\n",
    "    route_properties = {3: torch.empty((0, 3, 30)), 2: torch.empty((0, 2, 30))}\n",
    "    route_choices = {3: torch.empty(0), 2: torch.empty(0)}\n",
    "\n",
    "    for i in range(data[\"car_av\"].shape[0]):\n",
    "        if data[\"car_av\"][i] == 1:\n",
    "            # 3 routes possible\n",
    "            arr_tensor_x = torch.from_numpy(data[\"x\"][i]).unsqueeze(0)\n",
    "            arr_tensor_z = torch.stack([torch.from_numpy(data['z'][i])]*3, dim=0).unsqueeze(0)\n",
    "            arr_tensor = torch.cat((arr_tensor_x, arr_tensor_z), dim=2)\n",
    "\n",
    "            # print(\"Arr.shape\", arr_tensor.shape, route_properties[3].shape)\n",
    "            route_properties[3] = torch.cat((route_properties[3], arr_tensor), dim=0)\n",
    "            route_choices[3] = torch.cat((route_choices[3], torch.tensor([data[\"y\"][i]-1])), dim=0)\n",
    "        else:\n",
    "            # 2 routes possible\n",
    "            arr_tensor_x = torch.from_numpy(data[\"x\"][i][:-1]).unsqueeze(0)\n",
    "            arr_tensor_z = torch.stack([torch.from_numpy(data['z'][i])]*2, dim=0).unsqueeze(0)\n",
    "            # print(\"Arr.shape\", arr_tensor_x.shape, arr_tensor_z.shape)\n",
    "            arr_tensor = torch.cat((arr_tensor_x, arr_tensor_z), dim=2)\n",
    "            \n",
    "            # print(\"Arr.shape\", arr_tensor.shape, route_properties[2].shape)\n",
    "            route_properties[2] = torch.cat((route_properties[2], arr_tensor), dim=0)\n",
    "            route_choices[2] = torch.cat((route_choices[2], torch.tensor([data[\"y\"][i]-1])), dim=0)\n",
    "    \n",
    "    temp_ds = CustomDataset(route_properties[num_route], route_choices[num_route])\n",
    "    sampler.append(SequentialSampler(Subset(temp_ds, np.arange(len(temp_ds)))))\n",
    "    ds.append(temp_ds)\n",
    "    \n",
    "    return ds, sampler\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, routes, labels):\n",
    "        self.routes = routes\n",
    "        self.choice = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.choice)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        choice = self.choice[idx]\n",
    "        routes = self.routes[idx]\n",
    "        sample = {\"route\": routes, \"choice\": choice}\n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/Users/jeremy/TasteNet-MNL/swissmetro/data/\"\n",
    "data_train = \"train_jeremy.pkl\"\n",
    "data_dev = \"dev_jeremy.pkl\"\n",
    "data_test = \"test_jeremy.pkl\"\n",
    "# data_all = \"swissmetro_all_jeremy.pkl\"\n",
    "from torch import nn, optim\n",
    "lr = 1e-5\n",
    "# optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_datasets = []\n",
    "dev_datasets = []\n",
    "test_datasets = []\n",
    "\n",
    "for k in range(2, 4):\n",
    "    ds_train, sampler_train = PrepareData(data_path, data_train, k)\n",
    "    ds_dev, sampler_dev = PrepareData(data_path, data_dev, k)\n",
    "    ds_test, sampler_test = PrepareData(data_path, data_test, k)\n",
    "    # ds_all, sampler_all = PrepareData(data_path, data_all, k)\n",
    "    train_datasets.append(ds_train)\n",
    "    dev_datasets.append(ds_dev)\n",
    "    test_datasets.append(ds_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEGEND\n",
    "0 TT\n",
    "1 HE\n",
    "2 CO\n",
    "3 MALE_0\n",
    "4 MALE_1\n",
    "5 AGE_0\n",
    "6 AGE_1\n",
    "7 AGE_2\n",
    "8 AGE_3\n",
    "9 AGE_4\n",
    "10 INCOME_0\n",
    "11 INCOME_1\n",
    "12 INCOME_2\n",
    "13 INCOME_3\n",
    "14 FIRST_0\n",
    "15 FIRST_1\n",
    "16 WHO_0\n",
    "17 WHO_1\n",
    "18 WHO_2\n",
    "19 PURPOSE_0\n",
    "20 PURPOSE_1\n",
    "21 PURPOSE_2\n",
    "22 PURPOSE_3\n",
    "23 LUGGAGE_0\n",
    "24 LUGGAGE_1\n",
    "25 LUGGAGE_2\n",
    "26 GA_0\n",
    "27 GA_1\n",
    "28 SM_SEATS_0\n",
    "29 SM_SEATS_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train():\n",
    "    batch_size = 256\n",
    "    NUM_EPOCHS = 10\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_correct = 0\n",
    "        train_total_seen = 0\n",
    "        train_total_loss = 0\n",
    "        dev_correct = 0\n",
    "        dev_total_seen = 0\n",
    "        dev_total_loss = 0\n",
    "        for k in range(2, 4):\n",
    "            train_loader = DataLoader(ConcatDataset(train_datasets[k-2]), batch_size=batch_size)\n",
    "            for i, data in enumerate(train_loader):\n",
    "                X = data[\"route\"].float().to(device)\n",
    "                y = data[\"choice\"].float().to(device)\n",
    "\n",
    "                # Begin training\n",
    "                # zero the parameter gradients\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(X, debug=False).to(device)\n",
    "                loss = criterion(y_pred, y.long())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                correct_batch = (torch.argmax(y_pred, dim=1) == y).float().sum().item()\n",
    "                train_correct += correct_batch\n",
    "                train_total_seen += len(y_pred)\n",
    "                train_total_loss += loss.item()\n",
    "            # model.eval()\n",
    "            # dev_loader = DataLoader(ConcatDataset(dev_datasets[k-2]), batch_size=batch_size)\n",
    "            # for i, data in enumerate(dev_loader):\n",
    "            #     X = data[\"route\"].float().to(device)\n",
    "            #     y = data[\"choice\"].float().to(device)\n",
    "            #     y_pred = model(X, debug=False).to(device)\n",
    "            #     correct_batch = (torch.argmax(y_pred, dim=1) == y).float().sum().item()\n",
    "            #     dev_correct += correct_batch\n",
    "            #     dev_total_seen += len(y_pred)\n",
    "            #     dev_total_loss += loss.item()\n",
    "        # print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Train Batch correct: \", correct_batch, len(y_pred))\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Train Total Loss: \", train_total_loss, \"Average Loss: \", train_total_loss/train_total_seen)\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Train Accuracy: \", 100*train_correct/train_total_seen,\"%\")\n",
    "        # print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Dev Batch correct: \", correct_batch, len(y_pred))\n",
    "        # print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Dev Total Loss: \", dev_total_loss, \"Average Loss: \", dev_total_loss/dev_total_seen)\n",
    "        # print(f\"Epoch {epoch+1}/{NUM_EPOCHS} Dev Accuracy: \", 100*dev_correct/dev_total_seen,\"%\")\n",
    "\n",
    "    # Test evaluation\n",
    "    test_correct = 0\n",
    "    test_total_seen = 0\n",
    "    test_total_loss = 0\n",
    "    for k in range(2, 4):\n",
    "        test_loader = DataLoader(ConcatDataset(test_datasets[k-2]), batch_size=batch_size)\n",
    "        for i, data in enumerate(test_loader):\n",
    "            X = data[\"route\"].float().to(device)\n",
    "            y = data[\"choice\"].float().to(device)\n",
    "            model.eval()\n",
    "            y_pred = model(X, debug=False).to(device)\n",
    "            correct_batch = (torch.argmax(y_pred, dim=1) == y).float().sum().item()\n",
    "            test_correct += correct_batch\n",
    "            test_total_seen += len(y_pred)\n",
    "            test_total_loss += loss.item()\n",
    "    # print(f\"Test Batch correct: \", correct_batch, len(y_pred))\n",
    "    print(f\"Test Total Loss: \", test_total_loss, \"Average Loss: \", test_total_loss/test_total_seen)\n",
    "    print(f\"Test Accuracy: \", 100*test_correct/test_total_seen,\"%\")\n",
    "    print(model)\n",
    "    print(model.conv1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 1/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 2/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 2/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 3/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 3/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 4/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 4/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 5/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 5/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 6/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 6/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 7/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 7/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 8/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 8/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 9/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 9/10 Train Accuracy:  36.718332442544096 %\n",
      "Epoch 10/10 Train Total Loss:  30.9199920296669 Average Loss:  0.004131479426732617\n",
      "Epoch 10/10 Train Accuracy:  36.718332442544096 %\n",
      "Test Total Loss:  7.678506016731262 Average Loss:  0.00478709851417161\n",
      "Test Accuracy:  36.78304239401496 %\n",
      "CNN1(\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(1, 30), stride=(1, 1), bias=False)\n",
      ")\n",
      "tensor([[[[ 0.0000,  0.0000, -0.0154,  0.0000,  0.0516,  0.0000,  0.1265,\n",
      "            0.0064,  0.0815, -0.1616,  0.0000,  0.0093, -0.1180,  0.0194,\n",
      "            0.0000,  0.0277,  0.0000,  0.0132,  0.1185,  0.0000,  0.0012,\n",
      "           -0.0463,  0.0580,  0.0000,  0.0891, -0.1147,  0.0000, -0.0616,\n",
      "            0.0000,  0.0645]]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "model = CNN1(num_features).to(device)\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 1/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 2/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 2/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 3/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 3/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 4/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 4/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 5/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 5/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 6/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 6/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 7/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 7/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 8/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 8/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 9/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 9/10 Train Accuracy:  25.16034206306788 %\n",
      "Epoch 10/10 Train Total Loss:  31.718872010707855 Average Loss:  0.004238224480319061\n",
      "Epoch 10/10 Train Accuracy:  25.16034206306788 %\n",
      "Test Total Loss:  7.929387092590332 Average Loss:  0.0049435081624628\n",
      "Test Accuracy:  22.693266832917704 %\n",
      "CNN2(\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(1, 495), stride=(1, 1), bias=False)\n",
      ")\n",
      "tensor([[[[-0.0180,  0.0000, -0.0164,  0.0000,  0.0013,  0.0000,  0.0396,\n",
      "           -0.0408, -0.0079, -0.0241,  0.0000, -0.0412,  0.0306, -0.0346,\n",
      "            0.0000,  0.0377,  0.0000,  0.0410, -0.0047,  0.0000, -0.0016,\n",
      "           -0.0202,  0.0227,  0.0000, -0.0399, -0.0258,  0.0000,  0.0205,\n",
      "            0.0000,  0.0030, -0.0235,  0.0421,  0.0399,  0.0089,  0.0079,\n",
      "           -0.0210,  0.0231,  0.0272, -0.0361,  0.0100, -0.0402, -0.0288,\n",
      "           -0.0326, -0.0123, -0.0251, -0.0097,  0.0023,  0.0409,  0.0368,\n",
      "            0.0191, -0.0210, -0.0043,  0.0048, -0.0390,  0.0387, -0.0295,\n",
      "            0.0324, -0.0380,  0.0263,  0.0064,  0.0298,  0.0425, -0.0084,\n",
      "           -0.0360,  0.0063,  0.0180, -0.0198, -0.0166,  0.0369, -0.0121,\n",
      "           -0.0076, -0.0154, -0.0208,  0.0270, -0.0227, -0.0097, -0.0131,\n",
      "            0.0429,  0.0061,  0.0279, -0.0012, -0.0409, -0.0399,  0.0076,\n",
      "            0.0095, -0.0014,  0.0277,  0.0348,  0.0189,  0.0406, -0.0434,\n",
      "           -0.0109, -0.0425, -0.0425, -0.0423, -0.0292,  0.0321, -0.0375,\n",
      "           -0.0144, -0.0354, -0.0180,  0.0319,  0.0346,  0.0090,  0.0182,\n",
      "            0.0446, -0.0413,  0.0346,  0.0327, -0.0270, -0.0059, -0.0079,\n",
      "           -0.0359, -0.0082, -0.0410,  0.0441,  0.0166,  0.0199,  0.0226,\n",
      "            0.0041,  0.0186,  0.0392, -0.0436,  0.0423, -0.0119,  0.0010,\n",
      "            0.0279,  0.0335, -0.0385, -0.0239, -0.0077,  0.0059,  0.0362,\n",
      "           -0.0352, -0.0083, -0.0164, -0.0266, -0.0180,  0.0194,  0.0107,\n",
      "            0.0100,  0.0123, -0.0044,  0.0295, -0.0366, -0.0056, -0.0385,\n",
      "            0.0340, -0.0034,  0.0160,  0.0199, -0.0110,  0.0312,  0.0178,\n",
      "           -0.0202, -0.0442, -0.0073, -0.0236,  0.0142, -0.0257, -0.0013,\n",
      "            0.0278,  0.0124, -0.0281,  0.0418,  0.0072, -0.0287, -0.0210,\n",
      "           -0.0209,  0.0139, -0.0172,  0.0176, -0.0334, -0.0068,  0.0244,\n",
      "           -0.0230, -0.0033, -0.0264, -0.0090,  0.0116, -0.0380, -0.0099,\n",
      "           -0.0078,  0.0265,  0.0158,  0.0189,  0.0441, -0.0038,  0.0059,\n",
      "            0.0075, -0.0266,  0.0076, -0.0239, -0.0245,  0.0318,  0.0173,\n",
      "           -0.0057, -0.0213,  0.0141, -0.0215,  0.0087,  0.0007, -0.0210,\n",
      "           -0.0328,  0.0432,  0.0087,  0.0073, -0.0333, -0.0038, -0.0002,\n",
      "           -0.0003, -0.0041,  0.0182,  0.0088, -0.0313, -0.0114, -0.0348,\n",
      "            0.0312,  0.0218,  0.0309, -0.0096, -0.0414,  0.0117, -0.0074,\n",
      "            0.0408, -0.0335, -0.0268, -0.0228,  0.0410,  0.0383,  0.0314,\n",
      "           -0.0419,  0.0031,  0.0008,  0.0242,  0.0435, -0.0301, -0.0186,\n",
      "            0.0237, -0.0403, -0.0347,  0.0056, -0.0018, -0.0375,  0.0032,\n",
      "           -0.0359, -0.0031, -0.0060, -0.0304, -0.0123,  0.0431, -0.0336,\n",
      "            0.0355,  0.0055,  0.0071,  0.0371,  0.0018, -0.0236,  0.0110,\n",
      "           -0.0275, -0.0317,  0.0103,  0.0036, -0.0321,  0.0268,  0.0099,\n",
      "           -0.0077, -0.0020, -0.0281,  0.0310,  0.0047, -0.0378,  0.0075,\n",
      "            0.0008,  0.0306,  0.0085, -0.0149, -0.0209,  0.0221,  0.0375,\n",
      "            0.0392, -0.0286, -0.0278,  0.0320,  0.0105,  0.0141, -0.0053,\n",
      "            0.0215,  0.0346,  0.0042,  0.0343, -0.0021,  0.0170,  0.0100,\n",
      "           -0.0128, -0.0099,  0.0303, -0.0020, -0.0195,  0.0266,  0.0373,\n",
      "            0.0148,  0.0226, -0.0175,  0.0236, -0.0053, -0.0203, -0.0448,\n",
      "           -0.0351,  0.0194,  0.0243,  0.0165,  0.0346, -0.0136,  0.0353,\n",
      "            0.0067, -0.0079,  0.0420,  0.0422, -0.0431, -0.0364,  0.0285,\n",
      "           -0.0109,  0.0411, -0.0112, -0.0284, -0.0179,  0.0043, -0.0364,\n",
      "            0.0258,  0.0121, -0.0235,  0.0143, -0.0113,  0.0062, -0.0388,\n",
      "           -0.0270,  0.0224,  0.0146, -0.0113,  0.0368,  0.0116, -0.0365,\n",
      "           -0.0199, -0.0315,  0.0255, -0.0413,  0.0057, -0.0400, -0.0038,\n",
      "            0.0283,  0.0104, -0.0182, -0.0265, -0.0360,  0.0328, -0.0208,\n",
      "            0.0053, -0.0291, -0.0271, -0.0254,  0.0209, -0.0198, -0.0117,\n",
      "           -0.0066, -0.0154,  0.0003,  0.0013, -0.0084, -0.0255,  0.0170,\n",
      "            0.0178,  0.0446, -0.0129, -0.0421, -0.0311,  0.0179,  0.0094,\n",
      "            0.0129, -0.0342, -0.0050,  0.0411, -0.0230,  0.0376,  0.0448,\n",
      "            0.0165,  0.0218,  0.0439,  0.0227, -0.0287,  0.0323,  0.0328,\n",
      "            0.0383,  0.0196, -0.0205, -0.0441,  0.0191,  0.0287, -0.0151,\n",
      "           -0.0371,  0.0391,  0.0316, -0.0046,  0.0140, -0.0212, -0.0194,\n",
      "           -0.0009,  0.0132,  0.0230, -0.0411,  0.0094,  0.0019,  0.0351,\n",
      "           -0.0321, -0.0112, -0.0230, -0.0161, -0.0204,  0.0243,  0.0192,\n",
      "            0.0338, -0.0202, -0.0379, -0.0133, -0.0365,  0.0085,  0.0102,\n",
      "            0.0366, -0.0119,  0.0309,  0.0365, -0.0435, -0.0079,  0.0086,\n",
      "            0.0180, -0.0355,  0.0187, -0.0077, -0.0254,  0.0229, -0.0244,\n",
      "           -0.0196,  0.0149, -0.0388,  0.0138,  0.0411,  0.0072,  0.0345,\n",
      "            0.0145, -0.0411, -0.0333, -0.0095, -0.0144,  0.0370,  0.0402,\n",
      "            0.0102,  0.0419,  0.0396, -0.0137, -0.0036,  0.0194,  0.0243,\n",
      "            0.0086, -0.0204,  0.0168,  0.0054, -0.0061,  0.0037,  0.0045,\n",
      "           -0.0049,  0.0413,  0.0202,  0.0310,  0.0315,  0.0148,  0.0231,\n",
      "           -0.0110, -0.0349,  0.0151, -0.0432, -0.0340, -0.0054, -0.0001,\n",
      "           -0.0245,  0.0437,  0.0024,  0.0383,  0.0374, -0.0140,  0.0241,\n",
      "            0.0359, -0.0165,  0.0056, -0.0407, -0.0161]]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "model = CNN2(num_features).to(device)\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Train Total Loss:  30.556217551231384 Average Loss:  0.004082872468096123\n",
      "Epoch 1/10 Train Accuracy:  42.00962052378407 %\n",
      "Epoch 2/10 Train Total Loss:  30.5024191737175 Average Loss:  0.004075684015729222\n",
      "Epoch 2/10 Train Accuracy:  42.811330839123464 %\n",
      "Epoch 3/10 Train Total Loss:  30.57978445291519 Average Loss:  0.0040860214394595395\n",
      "Epoch 3/10 Train Accuracy:  40.980758952431856 %\n",
      "Epoch 4/10 Train Total Loss:  30.570771753787994 Average Loss:  0.004084817177149652\n",
      "Epoch 4/10 Train Accuracy:  41.36825227151256 %\n",
      "Epoch 5/10 Train Total Loss:  30.45882886648178 Average Loss:  0.004069859549235941\n",
      "Epoch 5/10 Train Accuracy:  42.437199358631744 %\n",
      "Epoch 6/10 Train Total Loss:  30.476789474487305 Average Loss:  0.00407225941668724\n",
      "Epoch 6/10 Train Accuracy:  42.08979155531801 %\n",
      "Epoch 7/10 Train Total Loss:  30.466135263442993 Average Loss:  0.004070835818204568\n",
      "Epoch 7/10 Train Accuracy:  43.35916622127205 %\n",
      "Epoch 8/10 Train Total Loss:  30.55682474374771 Average Loss:  0.004082953600180079\n",
      "Epoch 8/10 Train Accuracy:  41.755745590593264 %\n",
      "Epoch 9/10 Train Total Loss:  30.531981766223907 Average Loss:  0.004079634121622649\n",
      "Epoch 9/10 Train Accuracy:  41.809192944949224 %\n",
      "Epoch 10/10 Train Total Loss:  30.50725084543228 Average Loss:  0.004076329615904901\n",
      "Epoch 10/10 Train Accuracy:  41.79583110636023 %\n",
      "Test Total Loss:  7.521112561225891 Average Loss:  0.004688972918470007\n",
      "Test Accuracy:  49.62593516209476 %\n",
      "Transformer(\n",
      "  (Transformer_encoder): TransformerEncoder(\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=495, out_features=495, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=495, out_features=495, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "          (linear2): Linear(in_features=495, out_features=495, bias=True)\n",
      "          (norm1): LayerNorm((495,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((495,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.2, inplace=False)\n",
      "          (dropout2): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (fc): Linear(in_features=495, out_features=495, bias=True)\n",
      "  )\n",
      "  (conv1): Conv2d(1, 1, kernel_size=(1, 498), stride=(1, 1), bias=False)\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=495)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "tensor([[[[-1.7276e-02, -4.4227e-02,  0.0000e+00,  2.6339e-03,  3.7598e-02,\n",
      "            2.5992e-02, -4.1113e-02, -1.9127e-02,  2.9634e-02, -2.6273e-02,\n",
      "            1.4632e-02, -2.1963e-02, -4.3721e-03,  3.0853e-02,  2.9245e-02,\n",
      "            3.8137e-02,  5.8366e-03, -2.0374e-02,  1.2076e-02, -2.6175e-02,\n",
      "            3.0349e-02,  2.0676e-02, -7.9804e-03, -9.6586e-03, -4.2846e-02,\n",
      "           -3.1135e-03,  3.8482e-02, -3.2362e-03,  1.2075e-02, -4.5131e-03,\n",
      "            4.9065e-03, -1.6263e-02, -1.1850e-04, -4.1395e-02,  3.6745e-03,\n",
      "           -3.1275e-02, -1.5227e-02, -3.1337e-02, -1.4417e-02, -1.2528e-02,\n",
      "            3.2058e-02,  1.0632e-02, -1.1613e-02,  1.2648e-03, -3.0265e-02,\n",
      "           -3.7340e-02, -4.1220e-02, -3.0425e-02,  8.0662e-03, -4.9682e-03,\n",
      "           -5.1044e-03, -1.1159e-02, -1.9726e-02,  2.2586e-02,  2.9992e-02,\n",
      "            6.1036e-03,  2.2405e-02, -2.8360e-02, -7.7933e-04, -1.4215e-02,\n",
      "           -6.3427e-03,  3.4999e-02,  2.8850e-02,  1.5429e-02, -3.3016e-02,\n",
      "            1.4116e-02, -1.7274e-02,  3.2991e-02,  1.7924e-02,  9.4563e-03,\n",
      "            6.2279e-03,  2.5958e-02,  4.4605e-02, -3.8415e-02,  1.4808e-02,\n",
      "           -1.6441e-03, -1.9880e-02,  2.6095e-02,  4.0214e-02,  2.2994e-02,\n",
      "           -4.2329e-02, -3.1041e-02,  1.8953e-03,  4.2259e-02,  3.0005e-02,\n",
      "           -1.5478e-02, -1.7719e-02, -4.2174e-02, -2.5837e-02,  5.3503e-03,\n",
      "            1.6076e-02,  1.2931e-02, -3.6370e-02, -3.5809e-02,  6.3483e-03,\n",
      "           -2.8429e-02, -1.4890e-02,  1.6651e-02,  1.2570e-02,  2.0377e-03,\n",
      "            2.3840e-04, -2.1945e-03,  1.7162e-02, -1.1599e-02, -3.9962e-02,\n",
      "           -1.0396e-02,  4.1603e-02,  4.4364e-02, -2.0129e-02,  3.4115e-02,\n",
      "            6.2068e-03,  9.8019e-03, -8.3593e-03, -4.1486e-02,  3.6985e-02,\n",
      "            1.8438e-02, -1.5193e-02,  2.8973e-03,  2.9428e-02,  1.7838e-02,\n",
      "            1.3475e-02,  2.4426e-02,  9.6961e-03,  3.3184e-02, -2.8648e-02,\n",
      "           -4.0759e-02,  4.2198e-03, -1.2958e-02,  2.0741e-02,  2.5775e-02,\n",
      "            1.7207e-02,  7.7031e-03, -3.6393e-03, -3.1910e-02,  7.2612e-03,\n",
      "           -3.3705e-02, -2.4972e-02, -3.5973e-02,  4.4670e-02, -2.0361e-02,\n",
      "           -8.0174e-03, -2.9100e-02, -3.1895e-02, -5.4815e-04, -1.3525e-03,\n",
      "            1.7777e-02, -4.4202e-02, -3.4217e-02,  3.0424e-02, -2.1235e-02,\n",
      "            3.5486e-02,  2.1934e-02,  3.7350e-02, -4.2804e-02, -2.6130e-02,\n",
      "           -2.2744e-02,  6.5755e-03, -3.9181e-02, -1.5242e-02, -1.7744e-02,\n",
      "           -4.3171e-02, -2.3126e-02,  2.3179e-02, -7.4889e-03,  2.0113e-02,\n",
      "            3.8705e-02,  8.2600e-03, -2.9875e-02,  6.6953e-03,  2.5304e-02,\n",
      "            4.3893e-02,  3.3132e-02, -3.4220e-03,  3.9300e-02,  2.0621e-02,\n",
      "           -3.9377e-02,  3.5243e-02,  2.1354e-02,  2.4341e-02,  3.9383e-02,\n",
      "            4.5423e-03,  2.3707e-02, -3.6571e-04, -4.1504e-02,  4.4413e-02,\n",
      "           -8.6936e-03, -3.2564e-02, -4.1418e-02,  1.0080e-02,  1.7252e-02,\n",
      "           -3.3310e-02,  9.4070e-03, -3.8958e-02,  1.8495e-02,  9.7733e-03,\n",
      "            1.1942e-02,  1.1105e-02,  1.2837e-02,  3.6799e-02,  1.0489e-02,\n",
      "           -3.3015e-03, -1.5897e-02, -3.7432e-02, -2.2321e-02, -2.6878e-02,\n",
      "            4.4142e-02, -4.0787e-02,  1.2038e-02,  3.8062e-02, -2.7368e-02,\n",
      "            1.0786e-02, -2.0157e-02, -8.1226e-03,  1.7561e-02,  3.7176e-02,\n",
      "           -9.1756e-03, -3.8018e-02, -8.8030e-03,  5.7011e-03,  4.0969e-02,\n",
      "           -2.2006e-02,  2.9488e-02, -4.1714e-02, -1.2814e-02, -1.4813e-02,\n",
      "           -7.3250e-03,  3.4238e-02, -3.8362e-02,  2.9559e-02, -2.5325e-02,\n",
      "            3.9610e-02,  2.7447e-02,  1.3998e-02,  3.5421e-02,  3.0071e-02,\n",
      "           -2.5997e-02, -4.0414e-02, -2.1105e-02, -3.0798e-02,  1.2239e-02,\n",
      "           -5.8329e-03,  1.8379e-02,  1.3719e-02, -9.6336e-03,  2.5814e-02,\n",
      "            1.4214e-02, -8.9281e-03,  4.3609e-02, -3.7837e-02,  1.6548e-02,\n",
      "            3.7529e-02, -1.3005e-02,  1.1939e-02, -4.2453e-02, -2.3009e-02,\n",
      "           -2.5870e-02,  4.3064e-02, -3.3552e-02, -3.2105e-02,  2.0075e-02,\n",
      "            2.0947e-02, -3.9828e-02, -2.5644e-02,  1.0944e-02, -1.6473e-02,\n",
      "            3.6373e-02,  2.5190e-03,  4.8003e-03, -8.8920e-03,  3.8933e-02,\n",
      "           -1.7710e-02,  3.8263e-02, -2.0805e-02,  1.3645e-02, -1.8508e-02,\n",
      "           -3.1535e-02,  3.5175e-02, -4.3700e-02,  3.2384e-02, -1.6276e-02,\n",
      "            1.2035e-02, -4.4096e-03, -2.3620e-02, -4.2235e-02, -3.3690e-02,\n",
      "           -3.0303e-02, -1.4436e-02, -4.3635e-02, -1.8332e-04,  4.1639e-02,\n",
      "           -3.6674e-02, -3.7459e-02, -1.8070e-02, -1.8382e-02,  1.4823e-02,\n",
      "            2.2881e-02,  3.7027e-02, -2.2604e-02,  2.5878e-02, -2.8086e-02,\n",
      "            1.5013e-02, -1.6526e-02,  2.3931e-02, -2.6105e-02, -2.4416e-03,\n",
      "            2.5627e-02, -9.5403e-04,  6.8590e-03, -7.1544e-03, -8.6998e-03,\n",
      "           -1.8125e-02, -3.5214e-02, -1.4716e-02,  1.3297e-03, -1.5137e-02,\n",
      "            3.1537e-02,  3.9002e-02, -1.5412e-02, -3.1730e-02, -3.6729e-02,\n",
      "           -8.7611e-03, -4.1196e-02,  1.0630e-03,  5.8200e-05,  3.4663e-02,\n",
      "            3.8176e-02, -1.2304e-02,  1.6636e-02, -2.7397e-02, -4.1885e-02,\n",
      "           -2.0694e-02, -2.5951e-02, -2.9834e-02, -3.3662e-03, -1.4438e-02,\n",
      "            1.8776e-02, -1.4129e-02, -1.1738e-02, -2.8535e-02, -2.0426e-02,\n",
      "           -3.6762e-03,  1.4770e-02, -1.9229e-04, -2.4052e-02, -4.1305e-03,\n",
      "            1.4273e-02, -3.7760e-03,  2.2194e-02, -4.0897e-02,  3.7654e-02,\n",
      "           -2.9331e-02, -8.0129e-03, -2.9265e-02,  2.3788e-02,  3.6035e-02,\n",
      "           -2.2207e-02,  7.7461e-03,  3.5352e-02, -4.0079e-02,  2.0069e-02,\n",
      "            9.3859e-03, -3.3068e-02, -3.7106e-02,  3.3351e-02,  1.8969e-02,\n",
      "           -3.4539e-02, -6.1274e-03,  1.7187e-02, -4.2680e-02, -3.9116e-02,\n",
      "            1.8714e-02, -1.2341e-03,  3.5183e-02, -3.6190e-02,  3.0421e-03,\n",
      "            2.8707e-02,  1.7621e-03,  9.4094e-03,  3.6283e-02,  2.6253e-02,\n",
      "            3.2449e-02, -2.9237e-02,  7.2455e-03,  3.8887e-02, -2.8804e-02,\n",
      "            2.4290e-02, -8.5222e-03, -1.5211e-03, -2.4813e-02, -3.7882e-02,\n",
      "            4.4749e-02,  3.6788e-02,  2.1582e-02, -5.9699e-04,  2.5962e-02,\n",
      "            3.4862e-02,  2.2226e-02, -2.0366e-02, -1.7173e-02,  2.2436e-02,\n",
      "            3.7620e-02, -1.9266e-02, -1.3427e-02, -1.1511e-02, -4.0828e-02,\n",
      "            4.3211e-02,  1.5745e-03,  2.4153e-02,  7.5844e-03, -6.1217e-03,\n",
      "            1.3835e-02, -1.0796e-02,  2.4879e-02,  4.3404e-02,  7.7980e-03,\n",
      "           -1.2571e-02,  4.2799e-02,  4.3091e-02,  2.1621e-02, -9.9878e-03,\n",
      "           -2.7139e-02,  1.0796e-02, -3.6006e-02,  2.7990e-02,  1.6243e-02,\n",
      "           -3.1227e-02, -1.5324e-02,  3.5609e-02, -6.8691e-03,  2.0518e-02,\n",
      "            1.9270e-02, -2.9632e-02,  4.1685e-02, -1.0180e-02, -1.9335e-02,\n",
      "           -6.0927e-03,  3.0327e-02,  3.2212e-02, -4.1267e-02, -1.0035e-02,\n",
      "            1.4782e-02,  2.5877e-03,  6.6488e-03,  4.3579e-04, -3.8801e-02,\n",
      "           -3.8245e-02, -1.8871e-02, -3.4307e-02,  3.3761e-02, -9.0614e-03,\n",
      "           -2.9408e-02, -1.3352e-02,  1.8664e-02,  4.0600e-03,  3.9103e-02,\n",
      "           -4.0754e-02,  2.1999e-02,  3.9904e-02, -8.5050e-03, -4.1124e-03,\n",
      "            4.3770e-02, -3.4381e-02, -5.8349e-03, -3.0723e-02,  1.1000e-02,\n",
      "           -4.3320e-02,  2.3509e-02,  5.5626e-03,  8.5662e-03,  2.8176e-02,\n",
      "           -4.0570e-02,  3.6759e-03, -1.9250e-02,  6.5647e-04, -4.2465e-02,\n",
      "           -1.0170e-02,  1.0362e-02, -2.9205e-02, -4.1709e-02, -7.7794e-03,\n",
      "            6.6514e-03, -4.2949e-02,  3.6798e-02, -2.8844e-02,  2.9268e-03,\n",
      "           -3.3606e-02, -3.0753e-02, -1.9569e-02, -1.2315e-03,  2.8621e-02,\n",
      "            1.4371e-02, -2.0287e-02,  4.4297e-02, -3.7147e-02, -9.0229e-03,\n",
      "           -2.3289e-02,  3.7728e-02,  1.0277e-02]]]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "model = Transformer(num_features, hidden_size, d_model, conv_kernel, nhead, num_encoder_layers, dim_feedforward, dropout).to(device)\n",
    "run_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
